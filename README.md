# Multimodal Multi-label Classification

Multimodal Multi-label Classification (MMC) refers to a category of learning tasks that involve predicting one or more labels based on two modalities of information: images and text. MMC tasks present a greater complexity level than traditional single-modality, single-label classification tasks. This is primarily due to the requirement for models to process and interpret data from two distinct sources simultaneously. Designing an effective MMC system presents significant challenges, as it necessitates the creation of a learning structure capable of handling and fusing disparate input sources. This fusion must produce a comprehensive feature representation that integrates visual information with semantic content. The design of such a structure requires a deep understanding of both the visual and textual modalities. Furthermore, multi-label tasks inherently have more difficulty than single-label. In single-label classification tasks, the prediction with the highest posterior probability is typically selected as the output. Conversely, in multi-label classification tasks, the model is tasked with making predictions for each label, thereby increasing the complexity of the task. An additional layer of complexity in multi-label classification tasks arises from the intricate underlying dependencies that exist across labels. For instance, the label 'boat' is often associated with the label 'water'. Learning such relationships and incorporating them into the model further increases the complexity of the task. Therefore, the multimodal nature and the multi-label aspect of MMC tasks add to the complexity, making them a challenging yet intriguing area for machine learning research and development.

This study aims to classify 30000 multi-label images with textual descriptions of various sizes comprising 18 distinct classes. Each image is associated with one or more classes and a corresponding caption. The study has two primary objectives: Firstly, the study aims to experiment with different techniques that fuse the extracted features from the image and caption together. Secondly, the study plans to design and optimize the classification heads' structure and find a suitable design to enhance the models' performance. The performance of each technique and design choice will be compared and discussed, with the $F_1$ score serving as the primary performance metric. After obtaining the experiment's performance measures, the study will thoroughly analyze and justify the multimodal model's performance.

In todayâ€™s society, people consume vast amounts of multimedia content across various modalities, such as text, image, audio, video, and 3D. In many applications, utilising multiple modalities is extremely important, as it enhances performance by leveraging complementary information from different modalities, resulting in improved accuracy and robustness. It enables a richer representation of data by integrating distinct aspects conveyed by different modalities, facilitating more nuanced and accurate classification. Multimodal classification also helps handle ambiguity in classification problems by overcoming the limitations of individual modalities. The additional modalities essentially serve as extra opinions, effectively disambiguating data. Furthermore, studying the techniques and methods that deal with real-world data, which are usually multimodal, is crucial, enabling effective handling of diverse data types. Studies like this enable the development of models capable of cross-modal understanding and reasoning with practical implications in various domains. 

Learning a good feature presentation from a multimodel is a challenging task, as one must aim to learn robust and generalized features. Therefore, a straightforward solution is to train the model with many parameters on the sizable dataset. However, due to the limited training data, the increase in model parameters will lead to an overfitting problem. Moreover, the model size is constrained to 100 MB. Therefore, instead of training the model from scratch, we leverage the pre-trained model training on the data set and fine-tune it with the downstream dataset. We found that CLIP is the model that satisfies this objective well, as CLIP is trained on 400 million images and shows an impressive transferable ability. Therefore, our method is inspired by the motivation and fine-tuning of the classification heads. This work explores different classifiers, loss functions and fusion methods.
